{"cells":[{"cell_type":"markdown","metadata":{"id":"g1-katl1lBnB"},"source":["# Automated Program Repair\n","\n","Automated Program Repair (APR) aims to fix faults in programs with as little human intervention as possible. To automatically generate patches (candidates to fix our programs) we normally base our efforts on the original buggy program. This approach takes into account that developers implement programs that are *almost* accurate."]},{"cell_type":"markdown","metadata":{"id":"B2Q-xhNDlTG3"},"source":["# Large Language Models\n","\n","Large language models (LLMs) are AI systems that are pre-trained on vast amounts of textual data, such as books, articles, websites and also code. These models use deep learning algorithms to analyze patterns and structures in both natural and programming languages. That way, they produce human-like text or code, generating coherent and relevant responses across various tasks.\n","Some example of models are:\n","\n","* **Natural language:** BERT, GPT, etc.\n","* **Programming language:** CodeBERT, CodeGPT, Codex\n","* **Chat assistants:** ChatGPT, YOU, HuggingChat\n","* **LLMs:** GPT-3, GPT-4, OPT-175B, BLOOM-176B.\n","\n","  Try OPT-175B online: https://opt.alpa.ai/"]},{"cell_type":"markdown","metadata":{"id":"JCE74bn2O37-"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIx1XMaM_zcY"},"outputs":[],"source":["!pip install transformers\n","!pip install openai\n","!git clone https://gitlab.com/FranciscoRibeiro/apr-llm-tutorial -b notebook\n","!mv apr-llm-tutorial/* .\n","!rm -r apr-llm-tutorial"]},{"cell_type":"markdown","metadata":{"id":"kO9cVezVmBWA"},"source":["# Mask Filling\n","\n","*Mask filling* is a task in which, during training, some tokens are hidden (masked) from input sequences and the model learns to predict the original values based on the context of the surrounding tokens.\n","\n","Before moving on, let's consider the following Java class for a college degree containing the enrolled students:\n","\n","\n","\n","```java\n","public class Degree {\n","    private List<Student> students;\n","    ...\n","    public boolean containsStudent(String id) {\n","        return students.stream()\n","                       .filter(x -> x.getId().equals(id))\n","                       .count() < 0; // wrong operator\n","    }\n","```\n","\n","The method `containsStudent` is incorrect. The comparison in line 7 should be if the number of students is **greater than zero** (`> 0`).\n","\n","We could address this issue by exploring different code alternatives for the place we suspect is wrong. Mask filling can help us with that.\n","\n","`codebert-base-mlm` is a pre-trained model we can use for the masked modeling objective.\n","The *transformers* library makes this process very easy:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8XY2lOhm_YH"},"outputs":[],"source":["from transformers import pipeline\n","\n","code = \".count() <mask> 0;\" # Buggy line 7: notice we mask the operator\n","\n","# Simple way to ask CodeBERT to fill in the mask\n","fill_mask = pipeline('fill-mask', model=\"microsoft/codebert-base-mlm\")\n","outputs = fill_mask(code)\n","\n","# Check the generations\n","print(\"\\nGenerations:\")\n","for o in outputs:\n","    print(o)"]},{"cell_type":"markdown","metadata":{"id":"LQ5U7gLWnpCs"},"source":["The output will look something like this:\n","```javascript\n","{'score': 0.7650061845779419, 'token': 5457, 'token_str': ' ='}, ...}\n","{'score': 0.10670557618141174, 'token': 45994, 'token_str': ' =='}, ...}\n","{'score': 0.059551902115345, 'token': 671, 'token_str': ' return'}, ...}\n","{'score': 0.024524779990315437, 'token': 8061, 'token_str': ' >'}, ...}\n","{'score': 0.008818789385259151, 'token': 49333, 'token_str': '!='}, ...}    \n","```\n","\n","-   `token_str` represents the string value that should replace\n","    `<mask>`, i.e. it is the predicted token;\n","\n","-   `score` is the likelihood the model associates to the predicted\n","    token, i.e. the confidence that this is the correct token;\n","\n","-   `token` is the unique ID for the predicted token. Internally, the\n","    model represents tokens in this form in a structure called the\n","    *vocabulary*.\n","\n","The majority of suggestions do not help us that much. Although the\n","operator `>` is ranked 4th, we can still do better. Mask filling works\n","by analyzing the surrounding context, so we could try and provide more\n","information so that the model can make better predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"ccfJdMlApTG-"},"source":["### Exercise 1:\n","Modify the *input sequence* in `code` so that predictions are more\n","adequate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QUQsVNppUy0"},"outputs":[],"source":["from transformers import pipeline\n","\n","# Edit the 'code' variable\n","code = \"...\"\n","\n","# Simple way to ask CodeBERT to fill in the mask\n","fill_mask = pipeline('fill-mask', model=\"microsoft/codebert-base-mlm\")\n","outputs = fill_mask(code)\n","\n","# Check the generations\n","print(\"\\nGenerations:\")\n","for o in outputs:\n","    print(o)"]},{"cell_type":"markdown","metadata":{"id":"DBv1aGsCpt76"},"source":["<details>\n","  <summary><font size=\"3\" color=\"white\"><b>Click to show/hide hints</b></font></summary>\n","\n","\n","Provide the whole statement:\n","```python\n","code = \"return students.stream().filter(x -> x.getId().equals(id)).count() <mask> 0;\"\n","```\n","\n","Provide the whole method:\n","```python\n","code = \"\"\"public boolean containsStudent(String id) {\n","        return students.stream().filter(x -> x.getId().equals(id)).count() <mask> 0;\n","    }\"\"\"\n","```"]},{"cell_type":"markdown","metadata":{"id":"vGFuj6iP1gET"},"source":["### Exercise 2:\n","*CodeBERT* was trained on a dataset containing Python, Javascript, Ruby,\n","Go, Java, and PHP. We can also use the *transformers* library to predict\n","multiple masks. Consider the Python function:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683817053423,"user":{"displayName":"Francisco Ribeiro","userId":"12754801732637755242"},"user_tz":-60},"id":"4ZFx-gg912nS"},"outputs":[],"source":["# Computes intersection of two lists\n","def find_common(list1, list2):\n","    common = []\n","    for elem in list1:\n","        if elem in list2 and elem not in common:\n","            list1.add(elem) # buggy line: 'list1.add' is incorrect\n","    return common"]},{"cell_type":"markdown","metadata":{"id":"mpytLnAY2Gfa"},"source":["Use the above function as an *input sequence* and place 3 `<mask>`'s\n","to replace the expression `list1.add`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HScgPp5n2PHL"},"outputs":[],"source":["from transformers import pipeline\n","\n","# Edit the 'code' variable\n","code = \"\"\"...\"\"\"\n","\n","# Simple way to ask CodeBERT to fill in the mask\n","fill_mask = pipeline('fill-mask', model=\"microsoft/codebert-base-mlm\")\n","outputs = fill_mask(code)\n","\n","# Check the generations\n","print(\"\\nGenerations:\")\n","for o in outputs:\n","    print(o)"]},{"cell_type":"markdown","metadata":{"id":"30JcmRqc3v2Q"},"source":["Fix the `find_common` function according to the predicted tokens and run the respective cell.\n","\n","After that, run the next cell to test the function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzuA45pI2hKM"},"outputs":[],"source":["from utils import *\n","test_find_common(find_common)"]},{"cell_type":"markdown","metadata":{"id":"HqlSzWgcQHAP"},"source":["# Open-ended Generation\n","\n","The process of open-ended generation involves using a language model to generate a sequence of tokens based on the provided input. This input context includes code that precedes the point from which the model is asked to start generating additional code. The generated code sequence essentially continues the original code piece and should be coherent and relevant to the context provided to the model. *CodeGPT* is a model based on GPT-2 that performs this task. Let's look at a real-world bug from a Java program this time:\n","\n","```java\n","505c505\n","... .toArray(new String[m.size()])); // bug\n","---\n","... .toArray(new String[q.getValue().size()])); // fix\n","```\n","\n","The previous *diff* output highlights the changes that line 505 needs in order to be fixed. However, in a real scenario, we don't have access to a program's correct version. Can we use the buggy code to generate the necessary patch? Let's consider the code up until the point where we want *CodeGPT* to start completing it. Discarded code is to the right of the cursor (`|`) character.\n","\n","```java\n","... .toArray(new String[|m.size()]));\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"NNrXWeQmb0cz"},"source":["File `prompts/atmosphere_65ce3...3437.java` reflects this.\n","\n","Run the following code block:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JB7MpoIwcElT"},"outputs":[],"source":["from generate import *\n","\n","setup()\n","\n","# filename to analyze\n","filename = \"prompts/atmosphere_65ce31321f736b92ef7b5a9f8890c314e1313437.java\"\n","tokenizer, model = load() # Load the tokenizer and the model separately\n","\n","# Utility function to generate completions for the input file\n","tokens_used, completions = complete_file(filename, tokenizer, model)\n","\n","print(tokens_used) # Show used context (default: previous 1000 tokens)\n","print(\"\\nGenerations:\") # Show generations\n","for c in completions:\n","    print(c)\n","  "]},{"cell_type":"markdown","metadata":{"id":"7fXYfKrlectX"},"source":["The output will look like this:\n","```\n","Generations:\n","0]));} return newM; }/*** Set the request parameters.** @param request\n","```\n","\n","Considering the output above, *CodeGPT* generates index `0` (zero) and some more code. It seems it's not helping us out-of-the-box. However, we can adjust the generation process by passing more parameters to the `complete_file` function."]},{"cell_type":"markdown","metadata":{"id":"orSm_IQfexDA"},"source":["### Exercise 3\n","Parameterize the code block below to output multiple generations (`n_generations` parameter).\n","\n","### Exercise 4\n","Introduce variability throughout the generations (`variability` parameter can be `True` or `False`).\n","\n","### Exercise 5\n","Stop generations at a specific point (`stop_at` can be set to a specific character).\n","\n","### Exercise 6 (*Optional*)\n","Experiment with the other examples of real-world bugs in `prompts/`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFI94vbLNQdG"},"outputs":[],"source":["from generate import *\n","\n","setup()\n","\n","# filename to analyze\n","filename = \"prompts/atmosphere_65ce31321f736b92ef7b5a9f8890c314e1313437.java\"\n","tokenizer, model = load() # Load the tokenizer and the model separately\n","\n","# Utility function to generate completions for the input file\n","tokens_used, completions = complete_file(filename, tokenizer, model\n","                                         # , n_generations= # int\n","                                         # , variability= # bool\n","                                         # , stop_at= # char\n","                            )\n","print(tokens_used) # Show used context (default: previous 1000 tokens)\n","print(\"\\nGenerations:\") # Show generations\n","for c in completions:\n","    print(c)\n","  "]},{"cell_type":"markdown","metadata":{"id":"bNdYKZMTdg3p"},"source":["# Prompt Engineering\n","\n","Prompt engineering refers to the process of designing the input text (prompt) in a way that conveys or guides the model into generating the desired response. This makes LLM's more flexible by enabling them to generate responses to a wide variety of tasks, as we have a way to have models produce coherent output to virtually any question. It's almost like mimicking artificial general intelligence (AGI). Consider the following Python function to calculate the first `n` prime numbers:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":296,"status":"ok","timestamp":1683818203226,"user":{"displayName":"Francisco Ribeiro","userId":"12754801732637755242"},"user_tz":-60},"id":"7zFLiXgoXR8z"},"outputs":[],"source":["def calculate_prime_numbers(n):\n","    prime_numbers = []\n","    i = 2\n","    while len(prime_numbers) <= n:\n","        is_prime = False\n","        for j in range(2, i):\n","            if i % j == 0:\n","                is_prime = True\n","                break\n","        if is_prime:\n","            prime_numbers.add(i)\n","        i += 1\n","    return prime_numbers"]},{"cell_type":"markdown","metadata":{"id":"F4gNMR7lXbnh"},"source":["The implementation is faulty. Can you spot the mistake(s)? If you want, take 2 minutes to fix it by yourself. Either way, let's see how we could use prompt engineering to have GPT-3 generate a correct version. Suppose the following template for a prompt. Instructions surrounded with `<...>` mean comment syntax should be used:\n","```\n","<programming language>\n","<suggest the code is buggy>\n","... buggy code ...\n","\n","<say we want the fixed version>\n","```"]},{"cell_type":"markdown","metadata":{"id":"pfrlASmueSpj"},"source":["### Exercise 7\n","Based on the previous template, build the corresponding prompt for\n","`calculate_prime_numbers`. (Don't delete `%%writefile ...`)"]},{"cell_type":"markdown","metadata":{"id":"ygpUyUuze6Rs"},"source":["<details>\n","  <summary><font size=\"3\" color=\"white\"><b>Click to show/hide hints</b></font></summary>\n","\n","The prompt should look something like this:\n","```python\n","# Python\n","# Buggy\n","def calculate_prime_numbers(n):\n","    prime_numbers = []\n","    i = 2\n","    while len(prime_numbers) <= n:\n","        is_prime = False\n","        for j in range(2, i):\n","            if i % j == 0:\n","                is_prime = True\n","                break\n","        if is_prime:\n","            prime_numbers.add(i)\n","        i += 1\n","    return prime_numbers\n","\n","# Fixed\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1Oa4u9mmZLR"},"outputs":[],"source":["%%writefile primes_prompt.txt\n","YOUR PROMPT HERE"]},{"cell_type":"markdown","metadata":{"id":"lE3zuRtFe7iO"},"source":["### Exercise 8\n","After saving the prompt you built above (by running the respective code block), place your OpenAI API key below and run the next code block:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXR7JvuHAFGK"},"outputs":[],"source":["%%writefile openai.api_key\n","YOUR API KEY HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTpLXHMwguDA"},"outputs":[],"source":["from gpt import *\n","\n","setup()\n","prompt_file = \"primes_prompt.txt\"\n","\n","response = mk_request(prompt_file, mode=\"complete\")\n","show_response(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiLswC1MXnbw"},"outputs":[],"source":["# Don't forget to edit and run the previous code\n","# with calculate_prime_number's definition\n","calculate_prime_numbers(10)"]},{"cell_type":"markdown","metadata":{"id":"Pfco0SxyEqbX"},"source":["Suppose we are now aware of the test cases the function should satisfy:\n","```\n","calculate_prime_numbers(5) == [2, 3, 5]\n","calculate_prime_numbers(18) == [2, 3, 5, 7, 11, 13, 17]\n","calculate_prime_numbers(23) == [2, 3, 5, 7, 11, 13, 17, 19, 23]\n","calculate_prime_numbers(31) == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31]\n","```\n","\n","According to the test cases, the function should **not** calculate the\n","first `n` prime numbers but the prime numbers <u>up to</u> `n`.\n","\n","### Exercise 9\n","Through prompt engineering, create an input that makes the model output\n","an intended implementation of `calculate_prime_numbers`.\n"]},{"cell_type":"markdown","metadata":{"id":"tCfadCYTFiuJ"},"source":["<details>\n","  <summary><font size=\"3\" color=\"white\"><b>Click to show/hide hints</b></font></summary>\n","\n","The prompt could look something like this:\n","```python\n","# Python\n","# Test cases\n","calculate_prime_numbers(5) == [2, 3, 5]\n","calculate_prime_numbers(18) == [2, 3, 5, 7, 11, 13, 17]\n","calculate_prime_numbers(23) == [2, 3, 5, 7, 11, 13, 17, 19, 23]\n","calculate_prime_numbers(31) == [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31]\n","\n","# Function passing the test cases\n","```\n","\n","Or, an alternative:\n","```python\n","#Python\n","#Buggy Code:\n","#compute the first n primes\n","def calculate_prime_numbers(n):\n","    prime_numbers = []\n","    i = 2\n","    while len(prime_numbers) <= n:\n","        is_prime = True\n","        for j in range(2, i):\n","            if i % j == 0:\n","                is_prime = False\n","                break\n","        if is_prime:\n","            prime_numbers.append(i)\n","        i += 1\n","    return prime_numbers\n","#Fixed Code:\n","#compute primes up to number n\n","```\n","\n","There are many ways to approach the problem. Try different ones."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683818095590,"user":{"displayName":"Francisco Ribeiro","userId":"12754801732637755242"},"user_tz":-60},"id":"3UjmfK0FFUXc","outputId":"a94b3eb7-4578-4ac9-b2fb-a23dee349080"},"outputs":[],"source":["%%writefile primes_tests_prompt.txt\n","YOUR PROMPT HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fk1u9wUPYI36"},"outputs":[],"source":["from gpt import *\n","\n","setup()\n","prompt_file = \"primes_tests_prompt.txt\"\n","\n","response = mk_request(prompt_file, mode=\"complete\")\n","show_response(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ot9ZbqTYWkl"},"outputs":[],"source":["# Don't forget to edit and run the previous code\n","# with calculate_prime_number's definition\n","calculate_prime_numbers(10)"]},{"cell_type":"markdown","metadata":{"id":"I9ZG-hoFGAss"},"source":["# Editing through Instructions\n","\n","There is another way to interact with GPT-3 through instructions (called *edit* mode) that are separate from the prompt. Sometimes, this can facilitate our efforts as we can isolate the changes we want to see without messing with the input.\n","\n","### Exercise 10\n","Consider the previous `calculate_prime_numbers` function and generate different alternatives by instructing GPT to:\n","- use the sieve of eratosthenes;\n","- use a generator function;\n","- use memoization.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAL-zhOFGpPO"},"outputs":[],"source":["# Fixed version; but you are free to experiment with any version you want\n","%%writefile primes_function_fixed.txt\n","def calculate_prime_numbers(n):\n","    prime_numbers = []\n","    i = 2\n","    while len(prime_numbers) < n:\n","        is_prime = True\n","        for j in range(2, i):\n","            if i % j == 0:\n","                is_prime = False\n","                break\n","        if is_prime:\n","            prime_numbers.append(i)\n","        i += 1\n","    return prime_numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fi6PqCveGXsh"},"outputs":[],"source":["from gpt import *\n","\n","setup()\n","prompt_file = \"primes_function_fixed.txt\"\n","\n","response = mk_request(prompt_file,\n","                      mode=\"\", # change mode (complete, edit or insert)\n","                      instructions=\"\") # If \"edit\", provide instructions\n","show_response(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a2_z2WdY_Ec"},"outputs":[],"source":["# Use this code block to try the generated implementations\n","def calculate_prime_numbers(n):\n","  # Some GPT-3 generated implementation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R147StovrWCg"},"outputs":[],"source":["calculate_prime_numbers(10)"]},{"cell_type":"markdown","metadata":{"id":"PgSBwr3KIcg3"},"source":["A faulty program does not necessarily crash or output incorrect results. As we saw in the previous exercises, we may wish to change an implementation so that it is more efficient. In a way, we can consider we are still repairing the program. What if an app makes your phone spend more energy than it has to? We can look at it as being faulty.\n","\n","Below, there is a method which does not take into account the battery status when performing a demanding operation over the network. This is taken from a real-world bug.\n","\n","### Exercise 11\n","Instruct GPT-3 to prevent sync if battery is low."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRPUMy7PI1qh"},"outputs":[],"source":["%%writefile energy_battery_prompt.txt\n","public void onPerformSync(Account account, Bundle extras, String authority, ContentProviderClient provider, SyncResult syncResult) {\n","\t\tConnectivityManager manager = (ConnectivityManager) context.getSystemService(Context.CONNECTIVITY_SERVICE);\n","\t\tNetworkInfo networkInfo = manager.getActiveNetworkInfo();\n","\t\t\n","\t\t// Don't try to sync if no network!\n","\t\tif(networkInfo == null || !networkInfo.isConnected() || Util.isOffline(context)) {\n","\t\t\tLog.w(TAG, \"Not running sync, not connected to network\");\n","\t\t\treturn;\n","\t\t}\n","\n","\t\t// Check if user wants to only sync on wifi\n","\t\tSharedPreferences prefs = Util.getPreferences(context);\n","\t\tif(prefs.getBoolean(Constants.PREFERENCES_KEY_SYNC_WIFI, true)) {\n","\t\t\tif(networkInfo.getType() == ConnectivityManager.TYPE_WIFI) {\n","\t\t\t\texecuteSync(context);\n","\t\t\t} else {\n","\t\t\t\tLog.w(TAG, \"Not running sync, not connected to wifi\");\n","\t\t\t}\n","\t\t} else {\n","\t\t\texecuteSync(context);\n","\t\t}\n","\t}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1DyDnl1I16e"},"outputs":[],"source":["from gpt import *\n","\n","setup()\n","prompt_file = \"energy_battery_prompt.txt\"\n","\n","response = mk_request(prompt_file,\n","                      mode=\"\", # change mode (complete, edit or insert)\n","                      instructions=\"\") # If \"edit\", provide instructions\n","show_response(response)"]},{"cell_type":"markdown","metadata":{"id":"WiMsn4nZKp2W"},"source":["# Filling in arbitrary tokens\n","\n","*Insert* mode in GPT-3 can be used to recreate the effects of mask filling\n","with an arbitrary number of tokens by allowing the user to insert tokens\n","into partially complete code. We are not limited to a single predicted\n","token and we can insert multiple tokens while still using only one\n","mask.\n","\n","### Exercise 12\n","Use any of the example programs you saw throughout this tutorial and\n","build prompts to add code in the middle of content. Note that GPT-3\n","expects an `[insert]` tag instead of a `<mask>` tag."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":545,"status":"ok","timestamp":1683818556756,"user":{"displayName":"Francisco Ribeiro","userId":"12754801732637755242"},"user_tz":-60},"id":"xo13WiaWK9T3","outputId":"ba996385-5acb-45d3-e4cf-c63ab91f3d07"},"outputs":[],"source":["# Build the prompt you like and make sure you have only one \"[insert]\" tag\n","# Below is just an example, edit it as you like\n","%%writefile exercise_12_prompt.txt\n","def foo(a, b):\n","  return [insert]\n","\n","# Should be true\n","foo(1,2) == 3\n","foo(5,5) == 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm2AVhLWK-3u"},"outputs":[],"source":["from gpt import *\n","\n","setup()\n","prompt_file = \"exercise_12_prompt.txt\"\n","\n","response = mk_request(prompt_file,\n","                      mode=\"insert\") # change mode (complete, edit or insert)\n","show_response(response)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN66Z5IgD8aJZgFX0vqi3h8","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
