import os
import measure_utils as measure_utils
from codecarbon import OfflineEmissionsTracker

class LLAMACPP:
    def __init__(self, llm_obj, label, prompt_file_path, filename, model_name, seed, 
                 max_tokens, top_p, temperature, benchmark_type, save_output_flag, output_counter_id, 
                 n_shot, pass_k, language=None):
        """
        Initializes the LLAMACPP class with the provided arguments.

        Args:
            llm_obj (Llama): The Llama object to be executed.
            label (str): Identifier for the LLM execution in the final CSV file.
            prompt_file_path (str): Path to the text file containing the prompt.
            filename (str): Name of the final CSV file to append content to.
            model_name (str): Path to the LLM model (e.g., path to the .gguf file).
            seed (int): Seed for generating reproducible results.
            max_tokens (int): Maximum number of tokens to be used by the LLM in the response.
            top_p (float): The top-p value for nucleus sampling.
            temperature (float): Sampling temperature for the LLM.
            benchmark_type (str): Type of benchmark being executed (useful for sample generation).
            save_output_flag (str): Flag to save outputs to files ("yes") or not ("no").
            output_counter_id (int): Execution counter for k generations in the pass@k metric.
            language (str, optional): Language for the benchmark (optional).
        """
        self.label = label
        self.llm_obj = llm_obj
        self.prompt_file_path = prompt_file_path
        self.filename = filename
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.top_p = top_p
        self.temperature = temperature
        self.benchmark_type = benchmark_type
        self.save_output_flag = save_output_flag
        self.language = language
        self.seed = seed
        self.output_counter_id = output_counter_id
        self.n_shot = n_shot
        self.pass_k = pass_k


    def read_prompt(self) -> str:
        """
        Reads the prompt from the .txt file and deletes the file afterward.

        Returns:
            str: The prompt content.
        """
        with open(self.prompt_file_path, 'r') as prompt_file:
            content = prompt_file.read()

        os.remove(self.prompt_file_path)
        return content

    def execute_llamacpp(self, prompt: str) -> str:
        """
        Executes the Llama object with the provided prompt.

        Args:
            prompt (str): The prompt to be passed to the LLAMACPP object.

        Returns:
            str: The output generated by LLAMACPP.
        """
        output = self.llm_obj(prompt=prompt, max_tokens=self.max_tokens, seed=self.seed, top_p=self.top_p, 
                              temperature=self.temperature, stop=["Q:"], echo=True)["choices"][0]["text"]

        return output

    def run(self):
        """
        Executes the LLM with performance measurements using CodeCarbon.
        """
        # Log the measurement information for benchmarking
        measure_utils.print_measure_information(self.model_name, self.label, self.n_shot, 
                                                self.output_counter_id, self.pass_k)

        # Read the input prompt
        prompt = self.read_prompt()

        # Initialize the emissions tracker and start measurement
        tracker = OfflineEmissionsTracker(country_iso_code="PRT")
        tracker.start()

        # Execute the LLM with the prompt and stop the tracker after execution
        output = self.execute_llamacpp(prompt)
        tracker.stop()

        def clean_output(output: str) -> str:
            """
            Cleans the generated output by removing the n-shot prompts from the response.

            Args:
                output (str): Raw output from the model.

            Returns:
                str: Cleaned output without the n-shot prompts.
            """
            last_index = output.rfind('A:\n')

            if last_index == -1:
                print("No 'A:\n' found in the output.")
                return ""

            cleaned_output = output[last_index + len('A:\n'):]
            return cleaned_output

        # Clean the output to remove any unwanted content
        output = clean_output(output)

        # Generate benchmark-specific samples based on the type
        if self.benchmark_type == "humaneval_x":
            measure_utils.generate_samples_humaneval_x(measure_utils.extract_llm_name(self.model_name), 
                                                       output, self.label, self.n_shot, self.language)
        elif self.benchmark_type == "mbpp":
            measure_utils.generate_samples_mbpp(measure_utils.extract_llm_name(self.model_name), 
                                                output, self.label, self.n_shot)

        # Save the output to a file if the save_output_flag is set to "yes"
        if self.save_output_flag == "yes":
            if self.benchmark_type == "humaneval_x":
                measure_utils.save_output_to_file(output, self.label, self.label, self.language, 
                                                  "returned_prompts", measure_utils.extract_llm_name(self.model_name), 
                                                  "humaneval_x", self.output_counter_id, self.n_shot)
            elif self.benchmark_type == "mbpp":
                measure_utils.save_output_to_file(output, (self.label).replace("/", "_"), 
                                                  (self.label).replace("/", "_"), None, "returned_prompts", 
                                                  measure_utils.extract_llm_name(self.model_name), 
                                                  "mbpp", self.output_counter_id, self.n_shot)

        # Append the measurement data to the CSV file
        measure_utils.add_measurement_to_csv(self.filename, measure_utils.extract_llm_name(self.model_name), 
                                             self.label, tracker)
