### Python classes to execute LLMs and LLMs used in the dissertation

In order to add support to another framework to execute LLMs (e.g. ollama, transformers, ...):

1. Create a new Python class (in the Python code below I named as `NEW_LLM_EXECUTOR_CLASS`) similar to `LLAMACPP`class presented in `llamacpp_wrapper.py` that executes LLMs with the Llama.cpp library using a Python wrapper with methods for:
- reading prompts from a text file
- execute LLMs
- measure energy and runtime metrics with a framework such as CodeCarbon

```python
class NEW_LLM_EXECUTOR_CLASS:
    def __init__(self, llm_obj, label, prompt_file_path, filename, model_name, seed, 
                 max_tokens, top_p, temperature, benchmark_type, save_output_flag, output_counter_id, 
                 n_shot, pass_k, language=None):
        """
        Initializes the NEW_LLM_EXECUTOR_CLASS class with the provided arguments.

        Args:
            llm_obj (NEW_LLM_EXECUTOR): The NEW_LLM_EXECUTOR object to be executed.
            label (str): Identifier for the LLM execution in the final CSV file.
            prompt_file_path (str): Path to the text file containing the prompt.
            filename (str): Name of the final CSV file to append content to.
            model_name (str): Path to the LLM model (e.g., path to the .gguf file).
            seed (int): Seed for generating reproducible results.
            max_tokens (int): Maximum number of tokens to be used by the LLM in the response.
            top_p (float): The top-p value for nucleus sampling.
            temperature (float): Sampling temperature for the LLM.
            benchmark_type (str): Type of benchmark being executed (useful for sample generation).
            save_output_flag (str): Flag to save outputs to files ("yes") or not ("no").
            output_counter_id (int): Execution counter for k generations in the pass@k metric.
            language (str, optional): Language for the benchmark (optional).
        """
        pass

    def read_prompt(self) -> str:
        """
        Reads the prompt from the .txt file and deletes the file afterward.

        Returns:
            str: The prompt content.
        """
        pass

    def execute_new_llm_executor_class(self, prompt: str) -> str:
        """
        Executes the NEW_LLM_EXECUTOR object with the provided prompt.

        Args:
            prompt (str): The prompt to be passed to the NEW_LLM_EXECUTOR_CLASS object.

        Returns:
            str: The output generated by NEW_LLM_EXECUTOR_CLASS.
        """
        pass

    def run(self):
        """
        Executes the LLM with performance measurements.
        """
        pass
```
2. Update `../supported_models.json` to include a JSON entry where the key is `NEW_LLM_EXECUTOR_CLASS`, and the values are paths to the LLMs (e.g., files with extensions `.gguf`, `.bin`, etc.).

```json
[
    {
        "LLAMACPP": [
            "llms/models/codellama-7b-instruct.Q5_K_M.gguf"
        ]
    },
    {
        "NEW_LLM_EXECUTOR_CLASS": [
            "first_llm_path",
            "second_llm_path"
        ]
    }
]
```
3. Update the `load_llm` function in `llms/utils.py` to include a function for loading the LLM into a Python object.

```python
def load_llm(llm_path: str, n_ctx: int, seed: int):
    ...
    if llm_family == "LLAMACPP":
        ...
    elif llm_family == "NEW_LLM_EXECUTOR_CLASS":
        pass
    
    return None
```

4. Update the `execute_llm` function in the `main.py` file to include the new Python class created in step 1.

```python
def execute_llm(llm_obj: Llama, task_id: int, prompt: str, llm_path: str, 
                CSV_FILENAME: str, max_tokens: int, top_p: float, temperature: float, 
                benchmark_type: str, save_output_flag: str, language: str, 
                seed: int, output_counter_id: int, n_shot: int, pass_k: int) -> None:
    ...
    if llm_family == "LLAMACPP":
        ...
    elif llm_family == "NEW_LLM_EXECUTOR_CLASS":
        pass
    else:
        print(f"There is no class capable of executing the LLM with the path {llm_path}")
        ...
```

### LLMs used in the dissertation

| Model Name                              | Download Link                                                                                                                 |
|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|
| codegeex4-all-9b-Q6_K_L                 | [https://huggingface.co/bartowski/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-Q6_K_L.gguf](https://huggingface.co/bartowski/codegeex4-all-9b-GGUF/resolve/main/codegeex4-all-9b-Q6_K_L.gguf) |
| codellama-7b-instruct.Q5_K_M            | [https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_M.gguf](https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q5_K_M.gguf) |
| deepseek-coder-6.7b-instruct.Q5_K_M     | [https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q5_K_M.gguf](https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q5_K_M.gguf) |
| Meta-Llama-3-8B-Instruct-Q6_K           | [https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf) |
| starling-lm-7b-alpha.Q5_K_S             | [https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/resolve/main/starling-lm-7b-alpha.Q5_K_S.gguf](https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/resolve/main/starling-lm-7b-alpha.Q5_K_S.gguf) |
| tess-10.7b-v2.0.Q6_K                    | [https://huggingface.co/DavidAU/Tess-10.7B-v2.0-Q6_K-GGUF/resolve/main/tess-10.7b-v2.0.Q6_K.gguf](https://huggingface.co/DavidAU/Tess-10.7B-v2.0-Q6_K-GGUF/resolve/main/tess-10.7b-v2.0.Q6_K.gguf) |
| orca-2-13b.Q3_K_M                       | [https://huggingface.co/TheBloke/Orca-2-13B-GGUF/resolve/main/orca-2-13b.Q3_K_M.gguf](https://huggingface.co/TheBloke/Orca-2-13B-GGUF/resolve/main/orca-2-13b.Q3_K_M.gguf)                 |
